"""
Standalone —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤ –≥–æ—Ä–æ–¥–æ–≤ (18,000+ —Å—Ç—Ä–æ–∫)
–ë–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç Streamlit - —Ç–æ–ª—å–∫–æ pandas, rapidfuzz, requests

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python process_large_dataset.py input.xlsx output.xlsx

–ò–ª–∏ –≤ –∫–æ–¥–µ:
    from process_large_dataset import process_cities_file
    process_cities_file("input.xlsx", "output.xlsx")
"""

import pandas as pd
import requests
from rapidfuzz import fuzz, process
import re
import sys
from typing import Dict, List, Tuple, Optional

# ============================================
# –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================

PREFERRED_MATCHES = {
    '–∏–≤–∞–Ω–æ–≤–æ': '–ò–≤–∞–Ω–æ–≤–æ (–ò–≤–∞–Ω–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–∫–∏—Ä–æ–≤': '–ö–∏—Ä–æ–≤ (–ö–∏—Ä–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–ø–æ–¥–æ–ª—å—Å–∫': '–ü–æ–¥–æ–ª—å—Å–∫ (–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '—Ç—Ä–æ–∏—Ü–∫': '–¢—Ä–æ–∏—Ü–∫ (–ú–æ—Å–∫–≤–∞)',
    '–∂–µ–ª–µ–∑–Ω–æ–≥–æ—Ä—Å–∫': '–ñ–µ–ª–µ–∑–Ω–æ–≥–æ—Ä—Å–∫ (–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–∏–π –∫—Ä–∞–π)',
    '–∫–∏—Ä–æ–≤—Å–∫': '–ö–∏—Ä–æ–≤—Å–∫ (–õ–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–∏—Å—Ç—Ä–∞': '–ò—Å—Ç—Ä–∞ (–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–∫—Ä–∞—Å–Ω–æ–≥–æ—Ä—Å–∫': '–ö—Ä–∞—Å–Ω–æ–≥–æ—Ä—Å–∫ (–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–∏—Å—Ç—Ä–∞, –¥–µ—Ä–µ–≤–Ω—è –ø–æ–∫—Ä–æ–≤—Å–∫–æ–µ': '–ü–æ–∫—Ä–æ–≤—Å–∫–æ–µ (–≥–æ—Ä–æ–¥—Å–∫–æ–π –æ–∫—Ä—É–≥ –ò—Å—Ç—Ä–∞)',
    '–¥–æ–º–æ–¥–µ–¥–æ–≤–æ': '–î–æ–º–æ–¥–µ–¥–æ–≤–æ (–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–∫–ª–∏–Ω': '–ö–ª–∏–Ω (–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
    '–æ–∫—Ç—è–±—Ä—å—Å–∫–∏–π': '–û–∫—Ç—è–±—Ä—å—Å–∫–∏–π (–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –õ—é–±–µ—Ä–µ—Ü–∫–∏–π —Ä–∞–π–æ–Ω)',
    '—Å–æ–≤–µ—Ç—Å–∫': '–°–æ–≤–µ—Ç—Å–∫ (–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å)',
}

# ============================================
# –§–£–ù–ö–¶–ò–ò –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò
# ============================================

def normalize_city_name(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞: —ë->–µ, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã"""
    if not text:
        return ""
    text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    text = text.lower().strip()
    text = re.sub(r'\s+', ' ', text)
    return text


def normalize_region_name(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    text = normalize_city_name(text)
    replacements = {
        '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è': '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥',
        '–º–æ—Å–∫–æ–≤—Å–∫–∞—è': '–º–æ—Å–∫–æ–≤',
        '–∫—É—Ä—Å–∫–∞—è': '–∫—É—Ä—Å–∫',
        '–∫–µ–º–µ—Ä–æ–≤—Å–∫–∞—è': '–∫–µ–º–µ—Ä–æ–≤',
        '—Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è': '—Å–≤–µ—Ä–¥–ª–æ–≤',
        '–Ω–∏–∂–µ–≥–æ—Ä–æ–¥—Å–∫–∞—è': '–Ω–∏–∂–µ–≥–æ—Ä–æ–¥',
        '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è': '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫',
        '—Ç–∞–º–±–æ–≤—Å–∫–∞—è': '—Ç–∞–º–±–æ–≤',
        '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–∞—è': '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫',
        '–æ–±–ª–∞—Å—Ç—å': '',
        '–æ–±–ª': '',
        '–∫—Ä–∞–π': '',
        '—Ä–µ—Å–ø—É–±–ª–∏–∫–∞': '',
        '—Ä–µ—Å–ø': '',
        '  ': ' '
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text.strip()


def extract_city_and_region(text: str) -> Tuple[str, Optional[str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –∏ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤"""
    text_lower = text.lower()

    city_prefixes = ['–≥.', '–ø.', '–¥.', '—Å.', '–ø–æ—Å.', '–¥–µ—Ä.', '—Å–µ–ª–æ', '–≥–æ—Ä–æ–¥', '–ø–æ—Å–µ–ª–æ–∫', '–¥–µ—Ä–µ–≤–Ω—è']

    if ',' in text:
        text = text.split(',')[0].strip()

    region_keywords = [
        '–æ–±–ª–∞—Å—Ç', '–∫—Ä–∞–π', '—Ä–µ—Å–ø—É–±–ª–∏–∫', '–æ–∫—Ä—É–≥',
        '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥', '–º–æ—Å–∫–æ–≤', '–∫—É—Ä—Å–∫', '–∫–µ–º–µ—Ä–æ–≤',
        '—Å–≤–µ—Ä–¥–ª–æ–≤', '–Ω–∏–∂–µ–≥–æ—Ä–æ–¥', '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '—Ç–∞–º–±–æ–≤',
        '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫'
    ]

    text_cleaned = text.strip()
    for prefix in city_prefixes:
        if text_cleaned.lower().startswith(prefix + ' '):
            text_cleaned = text_cleaned[len(prefix) + 1:].strip()
            break
        elif text_cleaned.lower().startswith(prefix):
            text_cleaned = text_cleaned[len(prefix):].strip()
            break

    words = text_cleaned.split()

    if len(words) == 1:
        return text_cleaned, None

    city_words = []
    region_words = []
    region_found = False

    for word in words:
        word_lower = word.lower()
        if not region_found and any(keyword in word_lower for keyword in region_keywords):
            region_found = True
            region_words.append(word)
        elif region_found:
            region_words.append(word)
        else:
            city_words.append(word)

    city = ' '.join(city_words) if city_words else text_cleaned
    region = ' '.join(region_words) if region_words else None

    return city, region

# ============================================
# –§–£–ù–ö–¶–ò–ò –†–ê–ë–û–¢–´ –° HH.RU API
# ============================================

def get_hh_areas() -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ HH.ru"""
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ HH.ru...")
    response = requests.get('https://api.hh.ru/areas')
    data = response.json()

    areas_dict = {}

    def parse_areas(areas, parent_name="", parent_id="", root_parent_id=""):
        for area in areas:
            area_id = area['id']
            area_name = area['name']

            current_root_id = root_parent_id if root_parent_id else parent_id if parent_id else area_id

            areas_dict[area_name] = {
                'id': area_id,
                'name': area_name,
                'parent': parent_name,
                'parent_id': parent_id,
                'root_parent_id': current_root_id
            }

            if 'areas' in area and area['areas']:
                parse_areas(area['areas'], area_name, area_id, current_root_id)

    parse_areas(data)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(areas_dict)} –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ HH.ru\n")
    return areas_dict

# ============================================
# –§–£–ù–ö–¶–ò–ò –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø
# ============================================

def get_candidates_by_word(client_city: str, hh_city_names: List[str], limit: int = 20) -> List[Tuple[str, float]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞"""
    if not client_city or not client_city.strip():
        return []

    words = client_city.split()
    if not words:
        return []

    first_word = normalize_city_name(words[0])

    candidates = []
    for city_name in hh_city_names:
        city_lower = normalize_city_name(city_name)
        if first_word in city_lower:
            score = fuzz.WRatio(normalize_city_name(client_city), city_lower)
            candidates.append((city_name, score))

    candidates.sort(key=lambda x: x[1], reverse=True)

    return candidates[:limit]


def smart_match_city(client_city: str, hh_city_names: List[str], hh_areas: Dict, threshold: int = 85) -> Tuple[Optional[Tuple], List[Tuple[str, float]]]:
    """–£–º–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏ —É—á–µ—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""

    city_part, region_part = extract_city_and_region(client_city)
    city_part_lower = normalize_city_name(city_part)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    if city_part_lower in PREFERRED_MATCHES:
        preferred_match = PREFERRED_MATCHES[city_part_lower]
        if preferred_match in hh_city_names:
            score = fuzz.WRatio(city_part_lower, normalize_city_name(preferred_match))
            word_candidates = get_candidates_by_word(city_part, hh_city_names)
            return (preferred_match, score, 0), word_candidates

    word_candidates = get_candidates_by_word(city_part, hh_city_names)

    if word_candidates and len(word_candidates) > 0 and word_candidates[0][1] >= threshold:
        best_candidate = word_candidates[0]
        return (best_candidate[0], best_candidate[1], 0), word_candidates

    if not word_candidates or (word_candidates and word_candidates[0][1] < threshold):
        return None, word_candidates

    exact_matches = []
    exact_matches_with_region = []

    for hh_city_name in hh_city_names:
        hh_city_base = normalize_city_name(hh_city_name.split('(')[0].strip())

        if city_part_lower == hh_city_base:
            if region_part:
                region_normalized = normalize_region_name(region_part)
                hh_normalized = normalize_region_name(hh_city_name)

                if region_normalized in hh_normalized:
                    exact_matches_with_region.append(hh_city_name)
                else:
                    exact_matches.append(hh_city_name)
            else:
                exact_matches.append(hh_city_name)

    if exact_matches_with_region:
        best_match = exact_matches_with_region[0]
        score = fuzz.WRatio(city_part_lower, normalize_city_name(best_match))
        return (best_match, score, 0), word_candidates
    elif exact_matches:
        best_match = exact_matches[0]
        score = fuzz.WRatio(city_part_lower, normalize_city_name(best_match))
        return (best_match, score, 0), word_candidates

    candidates = process.extract(
        city_part,
        hh_city_names,
        scorer=fuzz.WRatio,
        limit=10
    )

    if not candidates:
        return None, word_candidates

    candidates = [c for c in candidates if c[1] >= threshold]

    if not candidates:
        return None, word_candidates

    if len(candidates) == 1:
        return candidates[0], word_candidates

    best_match = None
    best_score = 0

    for candidate_name, score, _ in candidates:
        candidate_lower = normalize_city_name(candidate_name)
        adjusted_score = score

        candidate_city = normalize_city_name(candidate_name.split('(')[0].strip())

        if city_part_lower == candidate_city:
            adjusted_score += 50
        elif city_part_lower in candidate_city:
            adjusted_score += 30
        elif candidate_city in city_part_lower:
            adjusted_score += 20
        else:
            adjusted_score -= 30

        if region_part:
            region_normalized = normalize_region_name(region_part)
            candidate_normalized = normalize_region_name(candidate_name)

            if region_normalized in candidate_normalized:
                adjusted_score += 40
            elif '(' in candidate_name:
                adjusted_score -= 25

        len_diff = abs(len(candidate_city) - len(city_part_lower))
        if len_diff > 3:
            adjusted_score -= 20

        if len(candidate_city) > len(city_part_lower) + 4:
            adjusted_score -= 25

        if len(candidate_name) > 15 and len(city_part) > 15:
            adjusted_score += 5

        region_keywords = ['oblast', '–∫—Ä–∞–π', '—Ä–µ—Å–ø—É–±–ª–∏–∫', '–æ–∫—Ä—É–≥']
        client_has_region = any(keyword in city_part_lower for keyword in region_keywords)
        candidate_has_region = any(keyword in candidate_lower for keyword in region_keywords)

        if client_has_region and candidate_has_region:
            adjusted_score += 15
        elif client_has_region and not candidate_has_region:
            adjusted_score -= 15

        if adjusted_score > best_score:
            best_score = adjusted_score
            best_match = (candidate_name, score, _)

    return (best_match if best_match else candidates[0]), word_candidates


def match_cities_batch(original_df: pd.DataFrame, hh_areas: Dict, threshold: int = 85, batch_size: int = 1000) -> pd.DataFrame:
    """
    –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ä–æ–¥–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –±–∞—Ç—á–∞–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤.
    –í–ê–ñ–ù–û: –ë–µ–∑ Streamlit UI —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (progress_bar, status_text)
    """
    results = []
    hh_city_names = list(hh_areas.keys())

    first_col_name = original_df.columns[0]
    other_cols = original_df.columns[1:].tolist() if len(original_df.columns) > 1 else []

    seen_original_cities = {}
    seen_hh_cities = {}

    duplicate_original_count = 0
    duplicate_hh_count = 0

    total_rows = len(original_df)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
    for batch_start in range(0, total_rows, batch_size):
        batch_end = min(batch_start + batch_size, total_rows)
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫ {batch_start + 1}-{batch_end} –∏–∑ {total_rows}...")

        batch_df = original_df.iloc[batch_start:batch_end]

        for idx, row in batch_df.iterrows():
            if (idx - batch_start + 1) % 100 == 0:
                print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx - batch_start + 1}/{len(batch_df)} –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ")

            client_city = row[first_col_name]
            other_values = {col: row[col] for col in other_cols}

            if pd.isna(client_city) or str(client_city).strip() == "":
                results.append({
                    '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city,
                    '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None,
                    'ID HH': None,
                    '–†–µ–≥–∏–æ–Ω': None,
                    '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0,
                    '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ù–µ—Ç',
                    '–°—Ç–∞—Ç—É—Å': '‚ùå –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
                    'row_id': idx,
                    **other_values
                })
                continue

            client_city_original = str(client_city).strip()
            client_city_normalized = normalize_city_name(client_city_original)

            if client_city_normalized in seen_original_cities:
                duplicate_original_count += 1
                original_result = seen_original_cities[client_city_normalized]
                results.append({
                    '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                    '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': original_result['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'],
                    'ID HH': original_result['ID HH'],
                    '–†–µ–≥–∏–æ–Ω': original_result['–†–µ–≥–∏–æ–Ω'],
                    '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': original_result['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %'],
                    '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': original_result['–ò–∑–º–µ–Ω–µ–Ω–∏–µ'],
                    '–°—Ç–∞—Ç—É—Å': 'üîÑ –î—É–±–ª–∏–∫–∞—Ç (–∏—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)',
                    'row_id': idx,
                    **other_values
                })
                continue

            match_result, candidates = smart_match_city(client_city_original, hh_city_names, hh_areas, threshold)

            if match_result:
                matched_name = match_result[0]
                score = match_result[1]
                hh_info = hh_areas[matched_name]
                hh_city_normalized = normalize_city_name(hh_info['name'])

                is_changed = client_city_original.strip() != hh_info['name'].strip()
                change_status = '–î–∞' if is_changed else '–ù–µ—Ç'

                if hh_city_normalized in seen_hh_cities:
                    duplicate_hh_count += 1
                    city_result = {
                        '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                        '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'],
                        'ID HH': hh_info['id'],
                        '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                        '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1),
                        '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': change_status,
                        '–°—Ç–∞—Ç—É—Å': 'üîÑ –î—É–±–ª–∏–∫–∞—Ç (—Ä–µ–∑—É–ª—å—Ç–∞—Ç HH)',
                        'row_id': idx,
                        **other_values
                    }
                    results.append(city_result)
                    seen_original_cities[client_city_normalized] = city_result
                else:
                    status = '‚úÖ –¢–æ—á–Ω–æ–µ' if score >= 95 else '‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ'

                    city_result = {
                        '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                        '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'],
                        'ID HH': hh_info['id'],
                        '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                        '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1),
                        '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': change_status,
                        '–°—Ç–∞—Ç—É—Å': status,
                        'row_id': idx,
                        **other_values
                    }

                    results.append(city_result)
                    seen_original_cities[client_city_normalized] = city_result
                    seen_hh_cities[hh_city_normalized] = True
            else:
                city_result = {
                    '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                    '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None,
                    'ID HH': None,
                    '–†–µ–≥–∏–æ–Ω': None,
                    '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0,
                    '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ù–µ—Ç',
                    '–°—Ç–∞—Ç—É—Å': '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                    'row_id': idx,
                    **other_values
                }

                results.append(city_result)
                seen_original_cities[client_city_normalized] = city_result

    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é: {duplicate_original_count}")
    print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É HH: {duplicate_hh_count}")

    return pd.DataFrame(results)

# ============================================
# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò API
# ============================================

def process_cities_file(input_file: str, output_file: str, threshold: int = 85, batch_size: int = 1000):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ —Å –≥–æ—Ä–æ–¥–∞–º–∏.

    Args:
        input_file: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (.xlsx –∏–ª–∏ .csv)
        output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (.xlsx –∏–ª–∏ .csv)
        threshold: –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 85)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
    """
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ HH.ru
    hh_areas = get_hh_areas()

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ {input_file}...")

    if input_file.endswith('.xlsx'):
        df = pd.read_excel(input_file)
    elif input_file.endswith('.csv'):
        df = pd.read_csv(input_file, encoding='utf-8')
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .xlsx –∏ .csv")

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫\n")

    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –±–∞—Ç—á–∞–º–∏
    print("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...\n")
    result_df = match_cities_batch(
        original_df=df,
        hh_areas=hh_areas,
        threshold=threshold,
        batch_size=batch_size
    )

    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ {output_file}...")

    if output_file.endswith('.xlsx'):
        result_df.to_excel(output_file, index=False)
    elif output_file.endswith('.csv'):
        result_df.to_csv(output_file, index=False, encoding='utf-8')
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .xlsx –∏ .csv")

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")

    # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total = len(result_df)
    matched = len(result_df[result_df['–°—Ç–∞—Ç—É—Å'].str.contains('‚úÖ', na=False)])
    similar = len(result_df[result_df['–°—Ç–∞—Ç—É—Å'].str.contains('‚ö†Ô∏è', na=False)])
    not_found = len(result_df[result_df['–°—Ç–∞—Ç—É—Å'].str.contains('‚ùå', na=False)])

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total}")
    print(f"–¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (‚úÖ): {matched} ({matched/total*100:.1f}%)")
    print(f"–ü–æ—Ö–æ–∂–∏—Ö (‚ö†Ô∏è): {similar} ({similar/total*100:.1f}%)")
    print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ (‚ùå): {not_found} ({not_found/total*100:.1f}%)")

    return result_df


def process_in_chunks(input_file: str, output_file: str, chunk_size: int = 5000, batch_size: int = 1000):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (50,000+ —Å—Ç—Ä–æ–∫) –ø–æ —á–∞—Å—Ç—è–º —Å –∑–∞–ø–∏—Å—å—é –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ.
    –î–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–º–µ—â–∞—é—Ç—Å—è –≤ –ø–∞–º—è—Ç—å —Ü–µ–ª–∏–∫–æ–º.

    Args:
        input_file: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (.xlsx –∏–ª–∏ .csv)
        output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (.xlsx)
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5000)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
    """
    hh_areas = get_hh_areas()

    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {input_file} –ø–æ —á–∞–Ω–∫–∞–º —Ä–∞–∑–º–µ—Ä–æ–º {chunk_size}...\n")

    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —á–∞—Å—Ç—è–º–∏
    if input_file.endswith('.xlsx'):
        chunks = pd.read_excel(input_file, chunksize=chunk_size)
    elif input_file.endswith('.csv'):
        chunks = pd.read_csv(input_file, chunksize=chunk_size, encoding='utf-8')
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .xlsx –∏ .csv")

    all_results = []

    for i, chunk in enumerate(chunks):
        print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i+1} ({len(chunk)} —Å—Ç—Ä–æ–∫)...")

        result_chunk = match_cities_batch(chunk, hh_areas, threshold=85, batch_size=batch_size)
        all_results.append(result_chunk)

        print(f"‚úÖ –ß–∞–Ω–∫ {i+1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    final_result = pd.concat(all_results, ignore_index=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {output_file}...")
    final_result.to_excel(output_file, index=False)

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total = len(final_result)
    matched = len(final_result[final_result['–°—Ç–∞—Ç—É—Å'].str.contains('‚úÖ', na=False)])
    similar = len(final_result[final_result['–°—Ç–∞—Ç—É—Å'].str.contains('‚ö†Ô∏è', na=False)])
    not_found = len(final_result[final_result['–°—Ç–∞—Ç—É—Å'].str.contains('‚ùå', na=False)])

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total}")
    print(f"–¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (‚úÖ): {matched} ({matched/total*100:.1f}%)")
    print(f"–ü–æ—Ö–æ–∂–∏—Ö (‚ö†Ô∏è): {similar} ({similar/total*100:.1f}%)")
    print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ (‚ùå): {not_found} ({not_found/total*100:.1f}%)")

    return final_result

# ============================================
# CLI –ò–ù–¢–ï–†–§–ï–ô–°
# ============================================

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python process_large_dataset.py input.xlsx output.xlsx [threshold] [batch_size]")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python process_large_dataset.py cities.xlsx result.xlsx")
        print("  python process_large_dataset.py cities.csv result.csv 90 500")
        print("\n–ê—Ä–≥—É–º–µ–Ω—Ç—ã:")
        print("  input.xlsx    - –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (.xlsx –∏–ª–∏ .csv)")
        print("  output.xlsx   - –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (.xlsx –∏–ª–∏ .csv)")
        print("  threshold     - –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 85)")
        print("  batch_size    - –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)")
        sys.exit(1)

    input_file = sys.argv[1]
    output_file = sys.argv[2]
    threshold = int(sys.argv[3]) if len(sys.argv) > 3 else 85
    batch_size = int(sys.argv[4]) if len(sys.argv) > 4 else 1000

    try:
        process_cities_file(input_file, output_file, threshold, batch_size)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
