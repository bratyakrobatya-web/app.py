"""
–ú–æ–¥—É–ª—å –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è VR –ú—É–ª—å—Ç–∏—Ç—É–ª

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –£–º–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π
- –ú–∞—Å—Å–æ–≤–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ DataFrame
- –°–ª–∏—è–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —Å –≥–æ—Ä–æ–¥–∞–º–∏ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–µ–π
"""

from typing import Dict, List, Tuple, Optional
import pandas as pd
import streamlit as st
from rapidfuzz import fuzz, process

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª–µ–π
from modules.matching import (
    normalize_city_name,
    extract_city_and_region,
    get_candidates_by_word,
    PREFERRED_MATCHES,
    EXCLUDED_EXACT_MATCHES
)
from modules.data_processing import normalize_region_name
from modules.utils import get_russian_cities, check_if_changed


def smart_match_city(
    client_city: str,
    hh_city_names: List[str],
    hh_areas: Dict,
    threshold: int = 85
) -> Tuple[Optional[Tuple[str, float, int]], List[Tuple[str, int]]]:
    """
    –£–º–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏ —É—á–µ—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π

    –§—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ:
    1. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏—è (EXCLUDED_EXACT_MATCHES)
    2. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (PREFERRED_MATCHES)
    3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –Ω–∞—á–∞–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É (get_candidates_by_word)
    4. –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏–æ–Ω–∞
    5. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç fuzzy matching —Å RapidFuzz
    6. –ü—Ä–∏–º–µ–Ω—è–µ—Ç adjustments –∫ score –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤

    Args:
        client_city: –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        hh_city_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ HH.ru
        hh_areas: –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤ HH.ru
        threshold: –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (0-100), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 85

    Returns:
        Tuple[Optional[Tuple[str, float, int]], List[Tuple[str, int]]]:
            - –õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: (–Ω–∞–∑–≤–∞–Ω–∏–µ, score, index) –∏–ª–∏ None
            - –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: [(–Ω–∞–∑–≤–∞–Ω–∏–µ, score), ...]

    Examples:
        >>> areas = get_hh_areas()
        >>> cities = get_russian_cities(areas)
        >>> match, candidates = smart_match_city("–ú–æ—Å–∫–≤–∞", cities, areas)
        >>> if match:
        ...     print(f"–ù–∞–π–¥–µ–Ω–æ: {match[0]}, Score: {match[1]}")
    """
    city_part, region_part = extract_city_and_region(client_city)
    city_part_lower = normalize_city_name(city_part)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è - –≥–æ—Ä–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å
    if city_part_lower in EXCLUDED_EXACT_MATCHES:
        word_candidates = get_candidates_by_word(city_part, hh_city_names)
        return None, word_candidates

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    if city_part_lower in PREFERRED_MATCHES:
        preferred_match = PREFERRED_MATCHES[city_part_lower]
        if preferred_match in hh_city_names:
            score = fuzz.WRatio(city_part_lower, normalize_city_name(preferred_match))
            word_candidates = get_candidates_by_word(city_part, hh_city_names)
            return (preferred_match, score, 0), word_candidates

    word_candidates = get_candidates_by_word(city_part, hh_city_names)

    if word_candidates and len(word_candidates) > 0 and word_candidates[0][1] >= threshold:
        best_candidate = word_candidates[0]
        return (best_candidate[0], best_candidate[1], 0), word_candidates

    if not word_candidates or (word_candidates and word_candidates[0][1] < threshold):
        return None, word_candidates

    exact_matches = []
    exact_matches_with_region = []

    for hh_city_name in hh_city_names:
        hh_city_base = normalize_city_name(hh_city_name.split('(')[0].strip())

        if city_part_lower == hh_city_base:
            if region_part:
                region_normalized = normalize_region_name(region_part)
                hh_normalized = normalize_region_name(hh_city_name)

                if region_normalized in hh_normalized:
                    exact_matches_with_region.append(hh_city_name)
                else:
                    exact_matches.append(hh_city_name)
            else:
                exact_matches.append(hh_city_name)

    if exact_matches_with_region:
        best_match = exact_matches_with_region[0]
        score = fuzz.WRatio(city_part_lower, normalize_city_name(best_match))
        return (best_match, score, 0), word_candidates
    elif exact_matches:
        best_match = exact_matches[0]
        score = fuzz.WRatio(city_part_lower, normalize_city_name(best_match))
        return (best_match, score, 0), word_candidates

    candidates = process.extract(
        city_part,
        hh_city_names,
        scorer=fuzz.WRatio,
        limit=10
    )

    if not candidates:
        return None, word_candidates

    candidates = [c for c in candidates if c[1] >= threshold]

    if not candidates:
        return None, word_candidates

    if len(candidates) == 1:
        return candidates[0], word_candidates

    best_match = None
    best_score = 0

    for candidate_name, score, _ in candidates:
        candidate_lower = normalize_city_name(candidate_name)
        adjusted_score = score

        candidate_city = normalize_city_name(candidate_name.split('(')[0].strip())

        if city_part_lower == candidate_city:
            adjusted_score += 50
        elif city_part_lower in candidate_city:
            adjusted_score += 30
        elif candidate_city in city_part_lower:
            adjusted_score += 20
        else:
            adjusted_score -= 30

        if region_part:
            region_normalized = normalize_region_name(region_part)
            candidate_normalized = normalize_region_name(candidate_name)

            if region_normalized in candidate_normalized:
                adjusted_score += 40
            elif '(' in candidate_name:
                adjusted_score -= 25

        len_diff = abs(len(candidate_city) - len(city_part_lower))
        if len_diff > 3:
            adjusted_score -= 20

        if len(candidate_city) > len(city_part_lower) + 4:
            adjusted_score -= 25

        if len(candidate_name) > 15 and len(city_part) > 15:
            adjusted_score += 5

        region_keywords = ['oblast', '–∫—Ä–∞–π', '—Ä–µ—Å–ø—É–±–ª–∏–∫', '–æ–∫—Ä—É–≥']
        client_has_region = any(keyword in city_part_lower for keyword in region_keywords)
        candidate_has_region = any(keyword in candidate_lower for keyword in region_keywords)

        if client_has_region and candidate_has_region:
            adjusted_score += 15
        elif client_has_region and not candidate_has_region:
            adjusted_score -= 15

        if adjusted_score > best_score:
            best_score = adjusted_score
            best_match = (candidate_name, score, _)

    return (best_match if best_match else candidates[0]), word_candidates


def match_cities(
    original_df: pd.DataFrame,
    hh_areas: Dict,
    threshold: int = 85,
    sheet_name: Optional[str] = None
) -> Tuple[pd.DataFrame, int, int, int]:
    """
    –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ä–æ–¥–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏ –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤

    –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç DataFrame —Å –≥–æ—Ä–æ–¥–∞–º–∏ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥—ã–π –≥–æ—Ä–æ–¥ —Å HH.ru.
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º HH.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame.

    Args:
        original_df: –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame —Å –≥–æ—Ä–æ–¥–∞–º–∏ (–ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü = –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤)
        hh_areas: –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤ HH.ru
        threshold: –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (0-100), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 85
        sheet_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ (–¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤), –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ

    Returns:
        Tuple[pd.DataFrame, int, int, int]:
            - DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
            - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º HH
            - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    Columns –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ:
        - –ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        - –ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ
        - ID HH
        - –†–µ–≥–∏–æ–Ω
        - –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %
        - –ò–∑–º–µ–Ω–µ–Ω–∏–µ (–î–∞/–ù–µ—Ç)
        - –°—Ç–∞—Ç—É—Å (‚úÖ –¢–æ—á–Ω–æ–µ / ‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ / ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ / üîÑ –î—É–±–ª–∏–∫–∞—Ç / ‚ùå –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
        - row_id (–∏–Ω–¥–µ–∫—Å —Å—Ç—Ä–æ–∫–∏)
        - [–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏–∑ original_df]

    Examples:
        >>> areas = get_hh_areas()
        >>> df = pd.DataFrame({'–ì–æ—Ä–æ–¥': ['–ú–æ—Å–∫–≤–∞', '–°–ø–±', '–ï–∫–±']})
        >>> result_df, dup_orig, dup_hh, total_dup = match_cities(df, areas)
        >>> print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(result_df)} –≥–æ—Ä–æ–¥–æ–≤, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {total_dup}")
    """
    results = []
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –≥–æ—Ä–æ–¥–∞
    hh_city_names = get_russian_cities(hh_areas)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤
    first_col_name = original_df.columns[0]
    other_cols = original_df.columns[1:].tolist() if len(original_df.columns) > 1 else []

    seen_original_cities = {}
    seen_hh_cities = {}

    duplicate_original_count = 0
    duplicate_hh_count = 0

    # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫—ç—à, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö –≤–∫–ª–∞–¥–æ–∫

    progress_bar = st.progress(0)
    status_text = st.empty()

    for idx, row in original_df.iterrows():
        progress = (idx + 1) / len(original_df)
        progress_bar.progress(progress)
        status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1} –∏–∑ {len(original_df)} –≥–æ—Ä–æ–¥–æ–≤...")

        client_city = row[first_col_name]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        other_values = {col: row[col] for col in other_cols}

        if pd.isna(client_city) or str(client_city).strip() == "":
            results.append({
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None,
                'ID HH': None,
                '–†–µ–≥–∏–æ–Ω': None,
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0,
                '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ù–µ—Ç',
                '–°—Ç–∞—Ç—É—Å': '‚ùå –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
                'row_id': idx,
                **other_values  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            })
            continue

        client_city_original = str(client_city).strip()
        client_city_normalized = normalize_city_name(client_city_original)

        if client_city_normalized in seen_original_cities:
            duplicate_original_count += 1
            original_result = seen_original_cities[client_city_normalized]
            results.append({
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': original_result['–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ'],
                'ID HH': original_result['ID HH'],
                '–†–µ–≥–∏–æ–Ω': original_result['–†–µ–≥–∏–æ–Ω'],
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': original_result['–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %'],
                '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': original_result['–ò–∑–º–µ–Ω–µ–Ω–∏–µ'],
                '–°—Ç–∞—Ç—É—Å': 'üîÑ –î—É–±–ª–∏–∫–∞—Ç (–∏—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)',
                'row_id': idx,
                **other_values
            })
            continue

        match_result, candidates = smart_match_city(client_city_original, hh_city_names, hh_areas, threshold)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Å—Ç–∞–≤–Ω–æ–π –∫–ª—é—á –¥–ª—è –≤–∫–ª–∞–¥–æ–∫, –ø—Ä–æ—Å—Ç–æ–π –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        cache_key = (sheet_name, idx) if sheet_name else idx
        st.session_state.candidates_cache[cache_key] = candidates

        if match_result:
            matched_name = match_result[0]
            score = match_result[1]
            hh_info = hh_areas[matched_name]
            hh_city_normalized = normalize_city_name(hh_info['name'])

            is_changed = check_if_changed(client_city_original, hh_info['name'])
            change_status = '–î–∞' if is_changed else '–ù–µ—Ç'

            if hh_city_normalized in seen_hh_cities:
                duplicate_hh_count += 1
                city_result = {
                    '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                    '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'],
                    'ID HH': hh_info['id'],
                    '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                    '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1),
                    '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': change_status,
                    '–°—Ç–∞—Ç—É—Å': 'üîÑ –î—É–±–ª–∏–∫–∞—Ç (—Ä–µ–∑—É–ª—å—Ç–∞—Ç HH)',
                    'row_id': idx,
                    **other_values
                }
                results.append(city_result)
                seen_original_cities[client_city_normalized] = city_result
            else:
                status = '‚úÖ –¢–æ—á–Ω–æ–µ' if score >= 95 else '‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ'

                city_result = {
                    '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                    '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'],
                    'ID HH': hh_info['id'],
                    '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                    '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1),
                    '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': change_status,
                    '–°—Ç–∞—Ç—É—Å': status,
                    'row_id': idx,
                    **other_values
                }

                results.append(city_result)
                seen_original_cities[client_city_normalized] = city_result
                seen_hh_cities[hh_city_normalized] = True
        else:
            city_result = {
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None,
                'ID HH': None,
                '–†–µ–≥–∏–æ–Ω': None,
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0,
                '–ò–∑–º–µ–Ω–µ–Ω–∏–µ': '–ù–µ—Ç',
                '–°—Ç–∞—Ç—É—Å': '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                'row_id': idx,
                **other_values
            }

            results.append(city_result)
            seen_original_cities[client_city_normalized] = city_result

    progress_bar.empty()
    status_text.empty()

    total_duplicates = duplicate_original_count + duplicate_hh_count

    return pd.DataFrame(results), duplicate_original_count, duplicate_hh_count, total_duplicates


def merge_cities_files(
    df1: pd.DataFrame,
    df2: pd.DataFrame,
    hh_areas: Dict,
    threshold: int = 85
) -> Tuple[pd.DataFrame, Dict]:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–∞ —Ñ–∞–π–ª–∞ —Å –≥–æ—Ä–æ–¥–∞–º–∏ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–µ–π

    –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–≤–∞ DataFrame —Å –≥–æ—Ä–æ–¥–∞–º–∏, —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥—ã–π –≥–æ—Ä–æ–¥ —Å HH.ru
    –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —É–¥–∞–ª—è—è –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–∞–∫ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º, —Ç–∞–∫ –∏ –ø–æ
    —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º HH.

    Args:
        df1: –ü–µ—Ä–≤—ã–π DataFrame —Å –≥–æ—Ä–æ–¥–∞–º–∏
        df2: –í—Ç–æ—Ä–æ–π DataFrame —Å –≥–æ—Ä–æ–¥–∞–º–∏
        hh_areas: –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ HH.ru
        threshold: –ü–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è (0-100), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 85

    Returns:
        Tuple[pd.DataFrame, Dict]:
            - merged_df: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame –±–µ–∑ –¥—É–±–ª–µ–π
            - stats: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:
                - total_from_file1: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ 1
                - total_from_file2: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ 2
                - duplicates_removed: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–µ–π
                - unique_cities: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
                - merged_total: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ

    Columns –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ:
        - –ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        - –ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ
        - ID HH
        - –†–µ–≥–∏–æ–Ω
        - –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %
        - –ò—Å—Ç–æ—á–Ω–∏–∫ (–§–∞–π–ª 1 / –§–∞–π–ª 2)
        - –°—Ç–∞—Ç—É—Å (‚úÖ –¢–æ—á–Ω–æ–µ / ‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ / ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ)

    Examples:
        >>> areas = get_hh_areas()
        >>> df1 = pd.DataFrame({'–ì–æ—Ä–æ–¥': ['–ú–æ—Å–∫–≤–∞', '–°–ø–±']})
        >>> df2 = pd.DataFrame({'–ì–æ—Ä–æ–¥': ['–°–ø–±', '–ï–∫–±']})  # –°–ø–± - –¥—É–±–ª–∏–∫–∞—Ç
        >>> merged_df, stats = merge_cities_files(df1, df2, areas)
        >>> print(f"–§–∞–π–ª 1: {stats['total_from_file1']}, –§–∞–π–ª 2: {stats['total_from_file2']}")
        >>> print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_removed']}")
        >>> print(f"–ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {stats['unique_cities']}")
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –≥–æ—Ä–æ–¥–∞
    hh_city_names = get_russian_cities(hh_areas)

    # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
    seen_original_cities = {}  # –ü–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é
    seen_hh_cities = {}  # –ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É HH

    results = []
    stats = {
        'total_from_file1': len(df1),
        'total_from_file2': len(df2),
        'duplicates_removed': 0,
        'unique_cities': 0,
        'merged_total': 0
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    first_col_name_df1 = df1.columns[0]
    first_col_name_df2 = df2.columns[0]

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª
    st.info("üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞...")
    progress_bar = st.progress(0)

    for idx, row in df1.iterrows():
        progress = (idx + 1) / len(df1)
        progress_bar.progress(progress)

        client_city = row[first_col_name_df1]

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if pd.isna(client_city) or str(client_city).strip() == "":
            continue

        client_city_original = str(client_city).strip()
        client_city_normalized = normalize_city_name(client_city_original)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤–∏–¥–µ–ª–∏ –ª–∏ –º—ã —É–∂–µ —ç—Ç–æ—Ç –≥–æ—Ä–æ–¥
        if client_city_normalized in seen_original_cities:
            stats['duplicates_removed'] += 1
            continue

        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å HH
        match_result, candidates = smart_match_city(client_city_original, hh_city_names, hh_areas, threshold)

        if match_result:
            matched_name = match_result[0]
            score = match_result[1]
            hh_info = hh_areas[matched_name]
            hh_city_normalized = normalize_city_name(hh_info['name'])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É HH
            if hh_city_normalized in seen_hh_cities:
                stats['duplicates_removed'] += 1
                continue

            # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ—Ä–æ–¥
            city_result = {
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'],
                'ID HH': hh_info['id'],
                '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1),
                '–ò—Å—Ç–æ—á–Ω–∏–∫': '–§–∞–π–ª 1',
                '–°—Ç–∞—Ç—É—Å': '‚úÖ –¢–æ—á–Ω–æ–µ' if score >= 95 else '‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ'
            }

            results.append(city_result)
            seen_original_cities[client_city_normalized] = city_result
            seen_hh_cities[hh_city_normalized] = True
            stats['unique_cities'] += 1
        else:
            # –ì–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ HH, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
            city_result = {
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None,
                'ID HH': None,
                '–†–µ–≥–∏–æ–Ω': None,
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0,
                '–ò—Å—Ç–æ—á–Ω–∏–∫': '–§–∞–π–ª 1',
                '–°—Ç–∞—Ç—É—Å': '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ'
            }

            results.append(city_result)
            seen_original_cities[client_city_normalized] = city_result
            stats['unique_cities'] += 1

    progress_bar.empty()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ç–æ—Ä–æ–π —Ñ–∞–π–ª
    st.info("üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞...")
    progress_bar = st.progress(0)

    for idx, row in df2.iterrows():
        progress = (idx + 1) / len(df2)
        progress_bar.progress(progress)

        client_city = row[first_col_name_df2]

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if pd.isna(client_city) or str(client_city).strip() == "":
            continue

        client_city_original = str(client_city).strip()
        client_city_normalized = normalize_city_name(client_city_original)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤–∏–¥–µ–ª–∏ –ª–∏ –º—ã —É–∂–µ —ç—Ç–æ—Ç –≥–æ—Ä–æ–¥ (–∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Ä–∞–Ω–µ–µ –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ)
        if client_city_normalized in seen_original_cities:
            stats['duplicates_removed'] += 1
            continue

        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å HH
        match_result, candidates = smart_match_city(client_city_original, hh_city_names, hh_areas, threshold)

        if match_result:
            matched_name = match_result[0]
            score = match_result[1]
            hh_info = hh_areas[matched_name]
            hh_city_normalized = normalize_city_name(hh_info['name'])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É HH
            if hh_city_normalized in seen_hh_cities:
                stats['duplicates_removed'] += 1
                continue

            # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ—Ä–æ–¥
            city_result = {
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': hh_info['name'],
                'ID HH': hh_info['id'],
                '–†–µ–≥–∏–æ–Ω': hh_info['parent'],
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': round(score, 1),
                '–ò—Å—Ç–æ—á–Ω–∏–∫': '–§–∞–π–ª 2',
                '–°—Ç–∞—Ç—É—Å': '‚úÖ –¢–æ—á–Ω–æ–µ' if score >= 95 else '‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ–µ'
            }

            results.append(city_result)
            seen_original_cities[client_city_normalized] = city_result
            seen_hh_cities[hh_city_normalized] = True
            stats['unique_cities'] += 1
        else:
            # –ì–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ HH, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
            city_result = {
                '–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ': client_city_original,
                '–ò—Ç–æ–≥–æ–≤–æ–µ –≥–µ–æ': None,
                'ID HH': None,
                '–†–µ–≥–∏–æ–Ω': None,
                '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ %': 0,
                '–ò—Å—Ç–æ—á–Ω–∏–∫': '–§–∞–π–ª 2',
                '–°—Ç–∞—Ç—É—Å': '‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ'
            }

            results.append(city_result)
            seen_original_cities[client_city_normalized] = city_result
            stats['unique_cities'] += 1

    progress_bar.empty()

    stats['merged_total'] = len(results)

    return pd.DataFrame(results), stats
